{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84ca7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    BIOREACTOR ML QUICK START                          â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "STEP 1: ORGANIZE YOUR DATA\n",
      "--------------------------\n",
      "Create this folder structure:\n",
      "\n",
      "    bioreactor_data/\n",
      "    â”œâ”€â”€ run_001.csv\n",
      "    â”œâ”€â”€ run_002.csv\n",
      "    â”œâ”€â”€ ...\n",
      "    â””â”€â”€ metadata.csv\n",
      "\n",
      "Each run CSV should have columns like:\n",
      "    time, od600, ph, do, substrate, temperature\n",
      "\n",
      "metadata.csv should have:\n",
      "    run_id, strain, media, final_titer_g_l, harvest_time_h\n",
      "\n",
      "STEP 2: ADAPT THE CODE\n",
      "-----------------------\n",
      "Edit the load_run() method in BioreactorDataLoader to match your format.\n",
      "\n",
      "STEP 3: RUN THE WORKFLOW\n",
      "-------------------------\n",
      "    from bioreactor_ml_starter import main_workflow\n",
      "\n",
      "    features, results = main_workflow(\n",
      "        data_dir='./bioreactor_data',\n",
      "        metadata_file='./bioreactor_data/metadata.csv'\n",
      "    )\n",
      "\n",
      "STEP 4: INTERPRET RESULTS\n",
      "--------------------------\n",
      "Check the output:\n",
      "    - RÂ² score > 0.7: Good! Move to neural operators\n",
      "    - RÂ² score 0.4-0.7: Okay, but need more data or better features\n",
      "    - RÂ² score < 0.4: Check data quality, may need transfer learning\n",
      "\n",
      "STEP 5: ITERATE\n",
      "---------------\n",
      "    - Add more features if needed\n",
      "    - Try different time windows (12h, 48h instead of 24h)\n",
      "    - Include strain/media as categorical features\n",
      "    - Move to neural operators with mechanistic priors\n",
      "\n",
      "TROUBLESHOOTING\n",
      "---------------\n",
      "Q: \"No data loaded\"\n",
      "A: Check file paths and format in load_run()\n",
      "\n",
      "Q: \"Not enough samples\"\n",
      "A: Need at least 20 runs, preferably 50+\n",
      "\n",
      "Q: \"Poor predictions\"\n",
      "A: Check if features are informative, may need transfer learning\n",
      "\n",
      "Q: \"Missing final_titer\"\n",
      "A: Need experimental endpoint measurements in metadata\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "                     Ready to start? Let's go! ðŸš€\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bioreactor ML Starter Code\n",
    "===========================\n",
    "\n",
    "Quick-start template for building ML models on bioreactor data.\n",
    "Designed for speed and practicality, not elegance.\n",
    "\n",
    "Author: Your Name\n",
    "Date: Started [Date]\n",
    "Goal: Predict final titer from early measurements, 8-month timeline\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Data Loading & Cleaning (ADAPT TO YOUR DATA FORMAT)\n",
    "# ============================================================================\n",
    "\n",
    "class BioreactorDataLoader:\n",
    "    \"\"\"\n",
    "    Load and clean bioreactor data.\n",
    "    Adapt the load_run() method to your specific data format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.runs = {}\n",
    "        \n",
    "    def load_run(self, filepath: Path) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load one bioreactor run. \n",
    "        \n",
    "        ADAPT THIS to your data format:\n",
    "        - CSV from bioreactor software?\n",
    "        - Excel sheets?\n",
    "        - Database export?\n",
    "        - Custom format?\n",
    "        \"\"\"\n",
    "        # Example for CSV with columns: time, OD600, pH, DO, substrate, etc.\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Standardize column names\n",
    "            df.columns = df.columns.str.lower().str.strip()\n",
    "            \n",
    "            # Convert time to hours from start\n",
    "            if 'time' in df.columns or 'timestamp' in df.columns:\n",
    "                time_col = 'time' if 'time' in df.columns else 'timestamp'\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "                df['hours'] = (df[time_col] - df[time_col].min()).dt.total_seconds() / 3600\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filepath}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_all_runs(self, pattern: str = \"*.csv\") -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Load all runs matching pattern\"\"\"\n",
    "        files = list(self.data_dir.glob(pattern))\n",
    "        print(f\"Found {len(files)} files matching '{pattern}'\")\n",
    "        \n",
    "        for f in files:\n",
    "            run_id = f.stem\n",
    "            df = self.load_run(f)\n",
    "            if df is not None:\n",
    "                self.runs[run_id] = df\n",
    "        \n",
    "        print(f\"Successfully loaded {len(self.runs)} runs\")\n",
    "        return self.runs\n",
    "    \n",
    "    def get_metadata(self, metadata_file: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load metadata (strain info, media, final titer, etc.)\n",
    "        \n",
    "        Expected format (CSV):\n",
    "        run_id, strain, media, final_titer_g_L, harvest_time_h, notes\n",
    "        \"\"\"\n",
    "        if metadata_file is None:\n",
    "            metadata_file = self.data_dir / \"metadata.csv\"\n",
    "        \n",
    "        if Path(metadata_file).exists():\n",
    "            return pd.read_csv(metadata_file)\n",
    "        else:\n",
    "            print(f\"Warning: No metadata file found at {metadata_file}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "class BioreactorDataCleaner:\n",
    "    \"\"\"\n",
    "    Clean and preprocess bioreactor time-series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def align_time(self, df: pd.DataFrame, time_col: str = 'hours') -> pd.DataFrame:\n",
    "        \"\"\"Ensure time starts at 0\"\"\"\n",
    "        df = df.copy()\n",
    "        df[time_col] = df[time_col] - df[time_col].min()\n",
    "        return df\n",
    "    \n",
    "    def interpolate_missing(self, df: pd.DataFrame, method: str = 'linear') -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].interpolate(method=method)\n",
    "        return df\n",
    "    \n",
    "    def remove_outliers(self, df: pd.DataFrame, cols: List[str], \n",
    "                       n_std: float = 3) -> pd.DataFrame:\n",
    "        \"\"\"Remove outliers beyond n standard deviations\"\"\"\n",
    "        df = df.copy()\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                mean = df[col].mean()\n",
    "                std = df[col].std()\n",
    "                df[col] = df[col].clip(mean - n_std*std, mean + n_std*std)\n",
    "        return df\n",
    "    \n",
    "    def resample_uniform(self, df: pd.DataFrame, interval_hours: float = 0.5) -> pd.DataFrame:\n",
    "        \"\"\"Resample to uniform time intervals\"\"\"\n",
    "        df = df.set_index('hours')\n",
    "        df = df.resample(f'{interval_hours}H').mean().interpolate()\n",
    "        return df.reset_index()\n",
    "    \n",
    "    def clean_run(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Full cleaning pipeline\"\"\"\n",
    "        df = self.align_time(df)\n",
    "        df = self.interpolate_missing(df)\n",
    "        df = self.remove_outliers(df, ['od600', 'ph', 'do'])\n",
    "        return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "class BioreactorFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features for ML from time-series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_early_features(self, df: pd.DataFrame, \n",
    "                              time_window: float = 24.0) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract features from early time window.\n",
    "        \n",
    "        This is the key: predict final titer from early measurements!\n",
    "        \"\"\"\n",
    "        early_df = df[df['hours'] <= time_window]\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # OD600 features\n",
    "        if 'od600' in df.columns:\n",
    "            features['od_24h'] = early_df['od600'].iloc[-1] if len(early_df) > 0 else np.nan\n",
    "            features['od_max_24h'] = early_df['od600'].max()\n",
    "            features['od_mean_24h'] = early_df['od600'].mean()\n",
    "            \n",
    "            # Growth rate (exponential phase)\n",
    "            if len(early_df) > 5:\n",
    "                od_log = np.log(early_df['od600'].replace(0, np.nan).dropna())\n",
    "                if len(od_log) > 2:\n",
    "                    # Linear fit to log(OD) vs time\n",
    "                    time = early_df.loc[od_log.index, 'hours']\n",
    "                    growth_rate = np.polyfit(time, od_log, 1)[0]\n",
    "                    features['growth_rate'] = growth_rate\n",
    "        \n",
    "        # pH features\n",
    "        if 'ph' in df.columns:\n",
    "            features['ph_24h'] = early_df['ph'].iloc[-1] if len(early_df) > 0 else np.nan\n",
    "            features['ph_min_24h'] = early_df['ph'].min()\n",
    "            features['ph_drop_24h'] = early_df['ph'].iloc[0] - early_df['ph'].iloc[-1] if len(early_df) > 1 else 0\n",
    "        \n",
    "        # DO features  \n",
    "        if 'do' in df.columns:\n",
    "            features['do_24h'] = early_df['do'].iloc[-1] if len(early_df) > 0 else np.nan\n",
    "            features['do_min_24h'] = early_df['do'].min()\n",
    "        \n",
    "        # Substrate features\n",
    "        if 'substrate' in df.columns:\n",
    "            features['substrate_24h'] = early_df['substrate'].iloc[-1] if len(early_df) > 0 else np.nan\n",
    "            features['substrate_consumed_24h'] = early_df['substrate'].iloc[0] - early_df['substrate'].iloc[-1] if len(early_df) > 1 else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self, runs: Dict[str, pd.DataFrame],\n",
    "                           metadata: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"Extract features from all runs\"\"\"\n",
    "        \n",
    "        feature_list = []\n",
    "        \n",
    "        for run_id, df in runs.items():\n",
    "            features = self.extract_early_features(df)\n",
    "            features['run_id'] = run_id\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if metadata is not None and run_id in metadata['run_id'].values:\n",
    "                meta_row = metadata[metadata['run_id'] == run_id].iloc[0]\n",
    "                features['final_titer'] = meta_row.get('final_titer_g_l', np.nan)\n",
    "                features['strain'] = meta_row.get('strain', 'unknown')\n",
    "                features['media'] = meta_row.get('media', 'unknown')\n",
    "            \n",
    "            feature_list.append(features)\n",
    "        \n",
    "        feature_df = pd.DataFrame(feature_list)\n",
    "        print(f\"\\nExtracted features from {len(feature_df)} runs\")\n",
    "        print(f\"Feature columns: {feature_df.columns.tolist()}\")\n",
    "        \n",
    "        return feature_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Baseline ML Models\n",
    "# ============================================================================\n",
    "\n",
    "def build_baseline_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Build and compare baseline models.\n",
    "    Start simple before going to neural operators!\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression, Ridge\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "    \n",
    "    models = {\n",
    "        'Linear': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boost': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BASELINE MODEL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  RÂ² Score:  {r2:.3f}\")\n",
    "        print(f\"  RMSE:      {rmse:.3f} g/L\")\n",
    "        print(f\"  MAE:       {mae:.3f} g/L\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_predictions(y_test, results, save_path='baseline_comparison.png'):\n",
    "    \"\"\"Visualize prediction quality\"\"\"\n",
    "    n_models = len(results)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, result) in enumerate(results.items()):\n",
    "        ax = axes[idx]\n",
    "        y_pred = result['predictions']\n",
    "        \n",
    "        # Scatter plot: predicted vs actual\n",
    "        ax.scatter(y_test, y_pred, alpha=0.6, s=60)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(y_test.min(), y_pred.min())\n",
    "        max_val = max(y_test.max(), y_pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "        \n",
    "        ax.set_xlabel('Actual Titer (g/L)', fontsize=11)\n",
    "        ax.set_ylabel('Predicted Titer (g/L)', fontsize=11)\n",
    "        ax.set_title(f\"{name}\\nRÂ² = {result['r2']:.3f}\", fontsize=12, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Comparison plot saved to {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Complete Workflow\n",
    "# ============================================================================\n",
    "\n",
    "def main_workflow(data_dir: str, metadata_file: str = None):\n",
    "    \"\"\"\n",
    "    End-to-end workflow from raw data to trained models.\n",
    "    \n",
    "    This is your starting point. Run this and see what happens!\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BIOREACTOR ML PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"\\n[STEP 1] Loading bioreactor data...\")\n",
    "    loader = BioreactorDataLoader(data_dir)\n",
    "    runs = loader.load_all_runs(pattern=\"*.csv\")  # Adjust pattern as needed\n",
    "    metadata = loader.get_metadata(metadata_file)\n",
    "    \n",
    "    if len(runs) == 0:\n",
    "        print(\"ERROR: No data loaded. Check your data_dir and file format.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Clean data\n",
    "    print(\"\\n[STEP 2] Cleaning time-series data...\")\n",
    "    cleaner = BioreactorDataCleaner()\n",
    "    runs_clean = {run_id: cleaner.clean_run(df) for run_id, df in runs.items()}\n",
    "    \n",
    "    # Step 3: Extract features\n",
    "    print(\"\\n[STEP 3] Extracting features...\")\n",
    "    extractor = BioreactorFeatureExtractor()\n",
    "    features = extractor.extract_all_features(runs_clean, metadata)\n",
    "    \n",
    "    # Check if we have target variable\n",
    "    if 'final_titer' not in features.columns:\n",
    "        print(\"\\nWARNING: No 'final_titer' column found in metadata.\")\n",
    "        print(\"You need final titer measurements to train predictive models!\")\n",
    "        print(\"Features extracted:\", features.columns.tolist())\n",
    "        return features\n",
    "    \n",
    "    # Step 4: Prepare train/test split\n",
    "    print(\"\\n[STEP 4] Preparing train/test split...\")\n",
    "    \n",
    "    # Remove rows with missing target\n",
    "    features = features.dropna(subset=['final_titer'])\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_cols = [c for c in features.columns \n",
    "                   if c not in ['run_id', 'final_titer', 'strain', 'media']]\n",
    "    \n",
    "    X = features[feature_cols].fillna(features[feature_cols].mean())\n",
    "    y = features['final_titer']\n",
    "    \n",
    "    # Train/test split (80/20)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Features: {feature_cols}\")\n",
    "    \n",
    "    # Step 5: Build baseline models\n",
    "    print(\"\\n[STEP 5] Training baseline models...\")\n",
    "    results = build_baseline_models(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Step 6: Visualize\n",
    "    print(\"\\n[STEP 6] Creating visualizations...\")\n",
    "    plot_predictions(y_test.values, results)\n",
    "    \n",
    "    # Step 7: Feature importance\n",
    "    print(\"\\n[STEP 7] Analyzing feature importance...\")\n",
    "    best_model = results['Random Forest']['model']\n",
    "    importances = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    print(importances.head())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WORKFLOW COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Check baseline_comparison.png to see model performance\")\n",
    "    print(\"2. If results are good, move to neural operators + transfer learning\")\n",
    "    print(\"3. If results are poor, check data quality and feature engineering\")\n",
    "    \n",
    "    return features, results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: Quick Start Guide\n",
    "# ============================================================================\n",
    "\n",
    "def quick_start_guide():\n",
    "    \"\"\"\n",
    "    Print usage instructions.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    BIOREACTOR ML QUICK START                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "STEP 1: ORGANIZE YOUR DATA\n",
    "--------------------------\n",
    "Create this folder structure:\n",
    "\n",
    "    bioreactor_data/\n",
    "    â”œâ”€â”€ run_001.csv\n",
    "    â”œâ”€â”€ run_002.csv\n",
    "    â”œâ”€â”€ ...\n",
    "    â””â”€â”€ metadata.csv\n",
    "\n",
    "Each run CSV should have columns like:\n",
    "    time, od600, ph, do, substrate, temperature\n",
    "\n",
    "metadata.csv should have:\n",
    "    run_id, strain, media, final_titer_g_l, harvest_time_h\n",
    "\n",
    "STEP 2: ADAPT THE CODE\n",
    "-----------------------\n",
    "Edit the load_run() method in BioreactorDataLoader to match your format.\n",
    "\n",
    "STEP 3: RUN THE WORKFLOW\n",
    "-------------------------\n",
    "    from bioreactor_ml_starter import main_workflow\n",
    "    \n",
    "    features, results = main_workflow(\n",
    "        data_dir='./bioreactor_data',\n",
    "        metadata_file='./bioreactor_data/metadata.csv'\n",
    "    )\n",
    "\n",
    "STEP 4: INTERPRET RESULTS\n",
    "--------------------------\n",
    "Check the output:\n",
    "    - RÂ² score > 0.7: Good! Move to neural operators\n",
    "    - RÂ² score 0.4-0.7: Okay, but need more data or better features\n",
    "    - RÂ² score < 0.4: Check data quality, may need transfer learning\n",
    "\n",
    "STEP 5: ITERATE\n",
    "---------------\n",
    "    - Add more features if needed\n",
    "    - Try different time windows (12h, 48h instead of 24h)\n",
    "    - Include strain/media as categorical features\n",
    "    - Move to neural operators with mechanistic priors\n",
    "\n",
    "TROUBLESHOOTING\n",
    "---------------\n",
    "Q: \"No data loaded\"\n",
    "A: Check file paths and format in load_run()\n",
    "\n",
    "Q: \"Not enough samples\"\n",
    "A: Need at least 20 runs, preferably 50+\n",
    "\n",
    "Q: \"Poor predictions\"\n",
    "A: Check if features are informative, may need transfer learning\n",
    "\n",
    "Q: \"Missing final_titer\"\n",
    "A: Need experimental endpoint measurements in metadata\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                     Ready to start? Let's go! ðŸš€\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN IT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Print guide\n",
    "    quick_start_guide()\n",
    "    \n",
    "    # Example usage (uncomment and modify paths):\n",
    "    # features, results = main_workflow(\n",
    "    #     data_dir='./your_bioreactor_data',\n",
    "    #     metadata_file='./your_bioreactor_data/metadata.csv'\n",
    "    # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
